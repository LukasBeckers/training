{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1547c480-6c9a-4da0-a4c3-2ab83390baf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import hdbscan\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchsummary\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe5bd28-f982-4755-bc4a-8fe2c804b18e",
   "metadata": {},
   "source": [
    "# Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "19d96792-36f9-40d7-8dec-3d20316905a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_pickle(\"train.pk\")\n",
    "val = pd.read_pickle(\"val.pk\")\n",
    "test = pd.read_pickle(\"test.pk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33b944eb-6fe8-4a0b-a150-394be6a55815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[\"BookingCode\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "973347ec-a724-4a87-abb8-a0d6f30f69ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {i:  booking_code for i, booking_code in enumerate(train[\"BookingCode\"].unique().tolist())}\n",
    "booking_code_to_label = {v: k for k, v in label_dict.items()}\n",
    "\n",
    "# PartNumber\n",
    "part_number_dict = {i:  part_number for i, part_number in enumerate(train[\"PartNumber\"].unique().tolist())}\n",
    "part_number_to_label = {v: k for k, v in part_number_dict.items()}\n",
    "\n",
    "# hdbscanCluster\n",
    "cluster_dict = {i:  cluster_number for i, cluster_number in enumerate(train[\"hdbscanCluster\"].unique().tolist())}\n",
    "cluster_to_label = {v: k for k, v in cluster_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6a23fa85-3da0-4239-a6ff-83dc39f348d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pytorch datasets\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, preprocess_transforms=None):\n",
    "        self.raw_data = dataframe\n",
    "        self.preprocess_transforms = preprocess_transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        row = self.raw_data.iloc[idx]\n",
    "        \n",
    "        sample = {\n",
    "            \"text\": row[\"Description\"],\n",
    "            \"text_emb\": row[\"descriptionEmbeddings\"],\n",
    "            \"part_num\": part_number_to_label[row[\"PartNumber\"]],\n",
    "            \"price\": row[\"Price\"],\n",
    "            \"price_norm\": row[\"NormPrice\"],\n",
    "            \"cluster\": cluster_to_label[row[\"hdbscanCluster\"]]                   \n",
    "        }\n",
    "\n",
    "        if self.preprocess_transforms is not None:\n",
    "            sample = self.preprocess_transforms(sample)\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "181498ff-6800-4223-9fc8-85d3f3b3da34",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = CustomDataset(train)\n",
    "val_ds = CustomDataset(val)\n",
    "test_ds = CustomDataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e95123ee-31b1-4c2f-8535-f60e5b1ef3e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'PartNumber', 'Description', 'Count', 'SumPrice', 'BookingCode',\n",
       "       'DocumentId', 'descriptionEmbeddings', 'Price', 'NormPrice',\n",
       "       'hdbscanCluster'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360a2bce-35d1-46bf-a037-afef5772a737",
   "metadata": {},
   "source": [
    "# Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb548dc-66a8-4383-9172-9b2153291b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class classifierModel(nn.Module):\n",
    "    def __init__(self, device):\n",
    "        super(classifierModel, self).__init__()\n",
    "\n",
    "        self.device = device\n",
    "        \n",
    "        self.n_part_number = len(part_number_dict)\n",
    "        self.n_cluster = len(cluster_dict)\n",
    "        \n",
    "        self.part_number_emb = nn.Linear(in_features=len(part_number_dict), out_features=768, bias=False, device=self.device)\n",
    "        self.price_adapter = nn.Linear(in_features=1, out_features=768, bias=False, device=self.device)\n",
    "        self.cluster_emb = nn.Linear(in_features=len(cluster_dict), out_features=768, bias=False, device=self.device)\n",
    "\n",
    "        self.norm1 = nn.BatchNorm1d(768, device=self.device)\n",
    "        self.linear1 = nn.Linear(in_features=768, out_features=384, device=self.device)\n",
    "\n",
    "        self.norm2 = nn.BatchNorm1d(384, device=self.device)\n",
    "        self.linear2 = nn.Linear(in_features=384, out_features=len(label_dict), device=self.device) # Classification Head\n",
    "\n",
    "        self.relu = F.leaky_relu \n",
    "\n",
    "\n",
    "    def forward(self, sample: dict):\n",
    "        text_emb = torch.tensor(sample[\"text_emb\"]).to(self.device)\n",
    "        part_num = torch.tensor(sample[\"part_num\"]).to(self.device)\n",
    "        price = torch.tensor(sample[\"price_norm\"]).to(self.device)\n",
    "        cluster = torch.tensor(sample[\"cluster\"]).to(self.device)\n",
    "\n",
    "        part_emb = self.part_number_emb(F.one_hot(part_num, num_classes=self.n_part_number))\n",
    "        price_emb = self.cluster_adapter(price_emb)\n",
    "        cluster_emb = self.cluster_emb(F.one_hot(cluster, num_classes=self.n_cluster))\n",
    "\n",
    "        out = torch.sum(torch.tensor(text_emb, part_emb, price_emb, cluster_emb), dim=0)\n",
    "        \n",
    "\n",
    "        \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca9d0d8-a4f1-43f0-a9f7-6a0eb56865a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=10, lr=1e-4):\n",
    "    device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #model = torch.compile(model)\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            #print(\"Batch: \", i, end=\"\\r\")\n",
    "            images = batch['image'].to(device).float()\n",
    "            labels = batch['class'].to(device).long()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                images = batch['image'].to(device).float()\n",
    "                labels = batch['class'].to(device).long()\n",
    "                outputs = model(images)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        val_acc = correct / total\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Val Acc: {val_acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "training",
   "language": "python",
   "name": "training"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
